{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b7b03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "from re import finditer\n",
    "import glob\n",
    "import random\n",
    "import fsspec\n",
    "import json\n",
    "from random import randint, choice\n",
    "from collections import Counter\n",
    "import spacy, itertools\n",
    "import langid\n",
    "from nltk.corpus import stopwords\n",
    "import fsspec, os, gzip\n",
    "from faker import Faker\n",
    "from faker.providers import person, company, geo, address, ssn\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer, MarianMTModel, AutoTokenizer, pipeline\n",
    "import torch\n",
    "import sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f184c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_regex = [\n",
    "    (\"EMAIL_ADDRESS\", re.compile(\n",
    "        \"([a-z0-9!#$%&'*+\\/=?^_`{|.}~-]+@(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?)\")),\n",
    "    (\"AGE\", re.compile(\"\\S+ years old|\\S+\\-years\\-old|\\S+ year old|\\S+\\-year\\-old\")),\n",
    "    # (\"PHONE_NUMBER\"    , re.compile('((?:(?<![\\d-])(?:\\+?\\d{1,3}[-.\\s*]?)?(?:\\(?\\d{3}\\)?[-.\\s*]?)?\\d{3}[-.\\s*]?\\d{4}(?![\\d-]))|(?:(?<![\\d-])(?:(?:\\(\\+?\\d{2}\\))|(?:\\+?\\d{2}))\\s*\\d{2}\\s*\\d{3}\\s*\\d{4}(?![\\d-])))')),\n",
    "    # (\"PHONE_NUMBER\"    , re.compile('((?:(?:\\+?1\\s*(?:[.-]\\s*)?)?(?:\\(\\s*(?:[2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9])\\s*\\)|(?:[2-9]1[02-9]|[2-9][02-8]1|[2-9][02-8][02-9]))\\s*(?:[.-]\\s*)?)?(?:[2-9]1[02-9]|[2-9][02-9]1|[2-9][02-9]{2})\\s*(?:[.-]\\s*)?(?:[0-9]{4})(?:\\s*(?:#|x\\.?|ext\\.?|extension)\\s*(?:\\d+)?))')),\n",
    "    (\"STREET_ADDRESS\", re.compile(\n",
    "        '\\d{1,4} [\\w\\s]{1,20} (?:street|st|avenue|ave|road|rd|highway|hwy|square|sq|trail|trl|drive|dr|court|ct|park|parkway|pkwy|circle|cir|boulevard|blvd)\\W?(?=\\s|$)')),\n",
    "    (\"STREET_ADDRESS\", re.compile('\\b\\d{5}(?:[-\\s]\\d{4})?\\b')),\n",
    "    (\"STREET_ADDRESS\", re.compile('P\\.? ?O\\.? Box \\d+')),\n",
    "    (\"GOVT_ID\", re.compile(\n",
    "        '(?!000|666|333)0*(?:[0-6][0-9][0-9]|[0-7][0-6][0-9]|[0-7][0-7][0-2])[- ](?!00)[0-9]{2}[- ](?!0000)[0-9]{4}')),\n",
    "    (\"DISEASE\", re.compile(\"diabetes|cancer|HIV|AIDS|Alzheimer's|Alzheimer|heart disease\")),\n",
    "    (\"NORP\", re.compile(\"upper class|middle class|working class|lower class\")),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = {'Zimbabwe', 'Panama', 'the Soviet Union', 'Bahamas''Colombia', 'AMERICA', 'Norway',\n",
    "           'The United States of America', 'Indonesia', 'Switzerland', 'U.S.A.', 'Chile', 'U.K.', \"North Korea's\",\n",
    "           'USSR', 'Greece', 'N. Korea', 'Viet Nam', 'america', 'U.S', 'South Korea', 'Sweden', 'New Zealand',\n",
    "           'The United States', 'Pakistan', 'United States', 'Holland', 'Taiwan', 'Ukraine', 'Italy', 'South Africa',\n",
    "           'Turkey', 'Spain', 'the United States of America', 'Britain', 'Saudi Arabia', 'Argentina', 'Vietnam', 'Cuba',\n",
    "           'Syria', 'UK', 'Belgium', 'Iran', 'Latvia', 'India', 'France', 'Great Britain', 'El Salvador', 'Australia',\n",
    "           'Brazil', 'Venezuela', 'England', 'the Roman Empire', 'russia', 'Gaza', 'Costa Rica', 'Yugoslavia',\n",
    "           'the United Kingdom', 'the United States', 'USA', 'North Korea', 'U.S.', 'Canada', 'US', 'America', 'China',\n",
    "           'U.S.', 'Russia', 'Israel', 'Afghanistan', 'PRC', 'Mexico', 'Iraq', 'Japan', 'Germany', }\n",
    "public_figures = {'Alec Baldwin', 'Stephen Harper', 'Al Gore', 'tRump', 'Trump', 'Harper', 'Justin', 'Comey', 'Obama',\n",
    "                  'Gary Johnson', 'Lindsey Graham', 'Michelle Obama', 'Mark Twain', 'Jared Kushner', 'Vladimir Putin',\n",
    "                  'Putin', 'Osama bin Laden', 'Fidel Castro', 'Mitch McConnell', 'Elizabeth Warren', 'Hilary Clinton',\n",
    "                  'George Bush', 'John Paul II', 'Jimmy Carter', \"Hillary Clinton's\", 'Ann Coulter', 'Rush Limbaugh',\n",
    "                  'JFK', 'Richard Nixon', 'Cameron Sellers', 'Ted Cruz', 'Richard Nixon', 'Jill Stein', 'Brad Wall',\n",
    "                  'James Comey', 'Jeff Sessions', 'Loretta Lynch', 'Sean Spicer', 'Nancy Pelosi', 'Ronald Reagan',\n",
    "                  'Gray Davis', 'Donald J. Trump', 'Dan Sullivan', 'George W. Bush', 'George Soros', 'Lisa Murkowski',\n",
    "                  'Gore', 'Kim Jong Un', \"Donald Trump's\", 'Steve Bannon', 'Alex Jones', 'Paul Ryan', 'Trumps',\n",
    "                  'Clintons', 'John McCain', 'Hillary', 'Obama', 'Trump', 'Clinton', 'Trudeau', 'Donald Trump', 'Bush',\n",
    "                  'Jesus', 'Hillary Clinton', 'Justin Trudeau', 'Bernie Sanders', 'Reagan', 'Bill Clinton',\n",
    "                  'Pope Francis', 'Hitler', 'Nixon', 'Sarah Palin', 'Barack Obama', }\n",
    "NORP = {'Islam', 'Conservative', 'the Conservative Party', 'GOP', 'Christianity', 'Church', 'the Republican Party',\n",
    "        'the Democratic Party', 'the Catholic Church', 'the Liberal Party', 'the Tea Party', 'Republican Party',\n",
    "        'the Roman Catholic Church', 'UNION', 'MAGA', 'the Democrat Party', }\n",
    "swap_org = ['Ford', 'Facebook', 'CNN', 'ABC', 'NBC', 'CBS', 'Google', 'Amazon', 'AOL', 'NCR', 'GE', 'Enron']\n",
    "bantu_surnames = [\"Dlamini\", \"Gumede\", \"Hadebe\", \"Ilunga\", \"Kamau\", \"Khoza\", \"Lubega\", \"M'Bala\", \"Mabaso\", \"Mabika\",\n",
    "                  \"Mabizela\", \"Mabunda\", \"Mabuza\", \"Macharia\", \"Madima\", \"Madondo\", \"Mahlangu\", \"Maidza\", \"Makhanya\",\n",
    "                  \"Malewezi\", \"Mamba\", \"Mandanda\", \"Mandlate\", \"Mangwana\", \"Manjate\", \"Maponyane\", \"Mapunda\", \"Maraire\",\n",
    "                  \"Masango\", \"Maseko\", \"Masemola\", \"Masengo\", \"Mashabane\", \"Masire\", \"Masondo\", \"Masuku\", \"Mataka\",\n",
    "                  \"Matovu\", \"Mbala\", \"Mbatha\", \"Mbugua\", \"Mchunu\", \"Mkhize\", \"Mofokeng\", \"Mokonyane\", \"Mutombo\",\n",
    "                  \"Ncube\", \"Ndagire\", \"Ndhlovu\", \"Ndikumana\", \"Ndiritu\", \"Ndlovu\", \"Ndzinisa\", \"Ngcobo\", \"Nkomo\",\n",
    "                  \"Nkosi\", \"Nkurunziza\", \"Radebe\", \"Tshabalala\", \"Tshivhumbe\", \"Vila\"]\n",
    "\n",
    "faker_map = dict([(a.split(\"_\")[0], a) for a in [\n",
    "    'ar_AA',\n",
    "    'ar_PS',\n",
    "    'ar_SA',\n",
    "    'bg_BG',\n",
    "    'cs_CZ',\n",
    "    'de_AT',\n",
    "    'de_CH',\n",
    "    'de_DE',\n",
    "    'dk_DK',\n",
    "    'el_GR',\n",
    "    'en_GB',\n",
    "    'en_IE',\n",
    "    'en_IN',\n",
    "    'en_NZ',\n",
    "    'en_TH',\n",
    "    'en_US',\n",
    "    'es_CA',\n",
    "    'es_ES',\n",
    "    'es_MX',\n",
    "    'et_EE',\n",
    "    'fa_IR',\n",
    "    'fi_FI',\n",
    "    'fr_CA',\n",
    "    'fr_CH',\n",
    "    'fr_FR',\n",
    "    'fr_QC',\n",
    "    'ga_IE',\n",
    "    'he_IL',\n",
    "    'hi_IN',\n",
    "    'hr_HR',\n",
    "    'hu_HU',\n",
    "    'hy_AM',\n",
    "    'id_ID',\n",
    "    'it_IT',\n",
    "    'ja_JP',\n",
    "    'ka_GE',\n",
    "    'ko_KR',\n",
    "    'lt_LT',\n",
    "    'lv_LV',\n",
    "    'ne_NP',\n",
    "    'nl_NL',\n",
    "    'no_NO',\n",
    "    'or_IN',\n",
    "    'pl_PL',\n",
    "    'pt_BR',\n",
    "    'pt_PT',\n",
    "    'ro_RO',\n",
    "    'ru_RU',\n",
    "    'sl_SI',\n",
    "    'sv_SE',\n",
    "    'ta_IN',\n",
    "    'th_TH',\n",
    "    'tr_TR',\n",
    "    'tw_GH',\n",
    "    'uk_UA',\n",
    "    'zh_CN',\n",
    "    'zh_TW']] + [('en', 'en_US')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb90dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5c926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_swap = True\n",
    "target_lang = 'id'\n",
    "texts = []\n",
    "ner_mappings = []\n",
    "row_ids = []\n",
    "domains = []\n",
    "lbracket = \"[\"\n",
    "rbracket = \"]\"\n",
    "if target_lang in ('zh', 'ja', 'ko'):\n",
    "    lbracket = \"[[\"\n",
    "    rbracket = \"]]\"\n",
    "faker_target_lang = Faker(faker_map[target_lang])\n",
    "faker_target_lang.add_provider(person)\n",
    "faker_target_lang.add_provider(geo)\n",
    "\n",
    "row_id = -1\n",
    "for s in tqdm(open(r\"C:\\Users\\Administrator\\pii_en.jsonl\", \"rb\")):\n",
    "    s = s.decode().strip()\n",
    "    if not s: continue\n",
    "    dat = json.loads(s)\n",
    "    domain = dat['domain']\n",
    "    ner = dat['ner']\n",
    "    text = dat['text']\n",
    "    if 'id' not in dat:\n",
    "        row_id += 1\n",
    "    else:\n",
    "        row_id = int(dat['id'])\n",
    "    if 'NYMEX' in text: continue\n",
    "    ner = [n for n in ner if n[0] not in (\"FREE\", \"â€™m\", 'Social Security')]\n",
    "    ner2 = []\n",
    "    if ' cancer ' in text:\n",
    "        ner2.append(['cancer', 'DISEASE'])\n",
    "    elif ' HIV ' in text:\n",
    "        ner2.append(['HIV', 'DISEASE'])\n",
    "    elif ' AIDS ' in text:\n",
    "        ner2.append(['AIDS', 'DISEASE'])\n",
    "    elif \"Alzheimer's\" in text:\n",
    "        ner2.append([\"Alzheimer's\", 'DISEASE'])\n",
    "    elif \"Alzheimer\" in text:\n",
    "        ner2.append(['Alzheimer', 'DISEASE'])\n",
    "    elif 'heart disease' in text:\n",
    "        ner2.append(['heart disease', 'DISEASE'])\n",
    "    elif 'democractic' in text:\n",
    "        ner2.append(['democractic', 'NORP'])\n",
    "    elif 'democrats' in text:\n",
    "        ner2.append(['democrats', 'NORP'])\n",
    "    elif 'Democrats' in text:\n",
    "        ner2.append(['Democrats', 'NORP'])\n",
    "    elif 'democrat' in text:\n",
    "        ner2.append(['democrat', 'NORP'])\n",
    "    elif 'Democrat' in text:\n",
    "        ner2.append(['Democrat', 'NORP'])\n",
    "    elif 'republican' in text:\n",
    "        ner2.append(['republican', 'NORP'])\n",
    "    elif 'republicans' in text:\n",
    "        ner2.append(['republicans', 'NORP'])\n",
    "    elif 'Republicans' in text:\n",
    "        ner2.append(['Republicans', 'NORP'])\n",
    "    elif 'republican' in text:\n",
    "        ner2.append(['republican', 'NORP'])\n",
    "    elif 'Republican' in text:\n",
    "        ner2.append(['Republican', 'NORP'])\n",
    "    elif 'socialist' in text:\n",
    "        ner2.append(['socialist', 'NORP'])\n",
    "    elif 'Socialist' in text:\n",
    "        ner2.append(['Socialist', 'NORP'])\n",
    "    for item in ner:\n",
    "        itemArr = item[0].split()\n",
    "        if itemArr[0] in ('Association', 'Society', 'Union') or itemArr[-1] in (\n",
    "                'Association', 'Society', 'Party'):\n",
    "            item[1] = 'NORP'\n",
    "        if item[0] == 'Enron':\n",
    "            item[0] = choice(swap_org)\n",
    "            text = text.replace(\"Enron\", item[0])\n",
    "            text = text.replace(\"@enron\", '@' + item[0].lower())\n",
    "            text = text.replace(\" enron\", ' ' + item[0].lower())\n",
    "            # print (text)\n",
    "        if item[0] in public_figures:\n",
    "            item[1] = 'PUBLIC_FIGURE'\n",
    "        elif item[0] in country:\n",
    "            item[1] = 'COUNTRY'\n",
    "        elif item[0] in NORP:\n",
    "            item[1] = 'NORP'\n",
    "        elif '@' in item[0]:\n",
    "            item[1] = 'EMAIL'\n",
    "        ner2.append(item)\n",
    "    # col.extend ([d[0] for d in ner2 if d[1] == 'NORP'])\n",
    "    context = {}\n",
    "    ner_mapping = {}\n",
    "    seen = {}\n",
    "    text = \"... \" + text + \" \"\n",
    "    text = text.replace(lbracket, \"(\")\n",
    "    text = text.replace(rbracket, \")\", )\n",
    "    if person_swap:\n",
    "        _idx = 0\n",
    "        for item in ner2:\n",
    "            if item[0] in seen: continue\n",
    "            seen[item[0]] = 1\n",
    "            if item[1] in ('COUNTRY', 'GPE', 'PERSON', 'GOVT_ID', 'STREET_ADDRESS',):  # ORG\n",
    "                text = text.replace(\" \" + item[0] + \" \", \" *\" + str(_idx) + \"* \")\n",
    "                text = text.replace(\" \" + item[0] + \",\", \" *\" + str(_idx) + \"* ,\")\n",
    "                text = text.replace(\" \" + item[0] + \"'\", \" *\" + str(_idx) + \"*'\")\n",
    "                text = text.replace(item[0], \"*\" + str(_idx) + \"*\")\n",
    "                context[item[0]] = context.get(item[0], \\\n",
    "                                               faker_target_lang.first_name() + \" \" + random.choice(\n",
    "                                                   bantu_surnames) if \" \" in item[0] and\n",
    "                                                                      item[\n",
    "                                                                          1] == 'PERSON' and target_lang in (\n",
    "                                                                          'yo', 'sw') else \\\n",
    "                                                   faker_target_lang.name() if \" \" in item[\n",
    "                                                       0] and item[\n",
    "                                                                                   1] == 'PERSON' else \\\n",
    "                                                       faker_target_lang.first_name() if\n",
    "                                                       item[\n",
    "                                                           1] == 'PERSON' else \\\n",
    "                                                           faker_target_lang.country() if\n",
    "                                                           item[\n",
    "                                                               1] == 'COUNTRY' else \\\n",
    "                                                               faker_target_lang.state() if\n",
    "                                                               item[\n",
    "                                                                   1] == 'GPE' and target_lang != 'zh' else \\\n",
    "                                                                   faker_target_lang.province() if\n",
    "                                                                   item[\n",
    "                                                                       1] == 'GPE' and target_lang == 'zh' else \\\n",
    "                                                                       faker_target_lang.ssn() if\n",
    "                                                                       item[\n",
    "                                                                           1] == 'GOVT_ID' else \\\n",
    "                                                                           faker_target_lang.address() if\n",
    "                                                                           item[\n",
    "                                                                               1] == 'STREET_ADDRESS' else \\\n",
    "                                                                               item[\n",
    "                                                                                   0])\n",
    "\n",
    "                ner_mapping[\"*\" + str(_idx) + \"*\"] = [context[item[0]],\n",
    "                                                      item[1] if item[\n",
    "                                                                     1] != 'COUNTRY' else 'GPE']\n",
    "                if \" \" in item[0]:\n",
    "                    context[item[0].split()[0]] = context[item[0]].split()[0]\n",
    "                    context[item[0].split()[-1]] = context[item[0]].split()[-1]\n",
    "                item[0] = context[item[0]]\n",
    "            else:\n",
    "                text = text.replace(item[0],\n",
    "                                    ' ' + str(_idx) + \" \" + lbracket + item[0] + rbracket)\n",
    "                ner_mapping[str(_idx) + \" \" + lbracket] = item\n",
    "            _idx += 1\n",
    "\n",
    "    texts.append(text)\n",
    "    ner_mappings.append(ner_mapping)\n",
    "    row_ids.append(row_id)\n",
    "    domains.append(domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88208491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Generate batches\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i: i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n",
    "model = model.to('cuda').half()\n",
    "translations = []\n",
    "for src_text_list in tqdm(chunks(texts, 16)):\n",
    "    batch = tokenizer(src_text_list, return_tensors=\"pt\", padding=True, truncation=True).to('cuda')\n",
    "    gen = model.generate(**batch, forced_bos_token_id=tokenizer.get_lang_id(target_lang))\n",
    "    outputs = tokenizer.batch_decode(gen, skip_special_tokens=True)\n",
    "    translations.extend(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275ad943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(('tr_hi'), 'wb') as f:\n",
    "    pickle.dump(translations, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dee581",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbracket = \"]\"\n",
    "if target_lang in ('zh', 'ja', 'ko'):\n",
    "    rbracket = \"]]\"\n",
    "with open(f\"pii_new_{target_lang}.jsonl\", \"w\", encoding=\"utf8\") as o:\n",
    "    for index, trans_text in enumerate(translations):\n",
    "        ner_found = []\n",
    "        trans_text = trans_text.lstrip(\".\")\n",
    "        trans_text = trans_text.strip()\n",
    "        trans_text = trans_text.replace(\"* \", \"*\").replace(\" *\", \"*\").replace('\"0*', '*0* ').replace(\n",
    "            '\"1*',\n",
    "            '*1* ').replace(\n",
    "            '\"2*', '*2* ').replace('\"3*', '*3* ').replace('\"4*', '*4* '). \\\n",
    "            replace('*0:', '*0* ').replace('*1:', '*1* ').replace('*2:', '*2* ').replace('*3:',\n",
    "                                                                                         '*3* ').replace(\n",
    "            '*4:', '*4* '). \\\n",
    "            replace('*0 ', '*0* ').replace('*1 ', '*1* ').replace('*2 ', '*2* ').replace('*3 ',\n",
    "                                                                                         '*3* ').replace(\n",
    "            '*4 ', '*4* '). \\\n",
    "            replace(' 0*', '*0* ').replace(' 1*', '*1* ').replace('2 *', '*2* ').replace(' 3*',\n",
    "                                                                                         '*3* ').replace(\n",
    "            ' 4*', '*4* ')\n",
    "        if trans_text.startswith('0') and not trans_text.startswith('0 ['):\n",
    "            trans_text = '*0* ' + trans_text[1:]\n",
    "        trans_text = trans_text.replace(\". [\", \" [\").replace(\".[\", \" [\").replace(\"  \", \" \")\n",
    "        orig_trans_text = trans_text\n",
    "        for key, ner_item in ner_mappings[index].items():\n",
    "            found = False\n",
    "            if key in trans_text:\n",
    "                found = True\n",
    "            elif key.replace(\" \", \"\") in trans_text:\n",
    "                found = True\n",
    "                key = key.replace(\" \", \"\")\n",
    "            elif key.lstrip('*') in trans_text:\n",
    "                found = True\n",
    "                key = key.lstrip('*')\n",
    "            elif key.rstrip('*') in trans_text:\n",
    "                found = True\n",
    "                key = key.rstrip('*')\n",
    "            if found:\n",
    "                if key[0] == '*':\n",
    "                    trans_text = trans_text.replace(key, \" \" + ner_item[0] + \" \")\n",
    "                    ner_found.append(list(ner_item))\n",
    "                else:\n",
    "                    trans_text2 = \"\"\n",
    "                    for segment in trans_text.split(key):\n",
    "                        if rbracket in segment:\n",
    "                            entity, rest = segment.split(rbracket, 1)\n",
    "                            entity = entity.strip(\"[]\")\n",
    "                            ner_found.append([entity, ner_item[1]])\n",
    "                            trans_text2 += \" \" + entity + \" \" + rest\n",
    "                        else:\n",
    "                            trans_text2 += \" \" + segment\n",
    "                    trans_text = trans_text2.strip()\n",
    "        trans_text = trans_text.replace(\"*\", \" \").replace(\"[\", \" \").replace(\"]\", \" \").replace(\n",
    "                \" .\",\n",
    "                \".\").replace(\" ,\",\n",
    "                             \",\").replace(\n",
    "                \"  \", \" \").replace(\"  \", \" \").strip()\n",
    "        if target_lang in ('zh', 'ja', 'ko'):\n",
    "            trans_text.replace(\" \", \"\")\n",
    "            trans_text.strip('#.')\n",
    "        if ner_found:\n",
    "            j = {'text': trans_text, 'ner': ner_found, 'domain': domains[index],\n",
    "                     'id': row_ids[index],\n",
    "                     'lang': target_lang}\n",
    "            print(j)\n",
    "            \n",
    "            o.write(json.dumps(j) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee9fdf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pii",
   "language": "python",
   "name": "pii"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
